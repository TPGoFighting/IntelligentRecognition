import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from models.pointnet2_sem_seg import get_model, get_loss
from core.dataset import TunnelDataset

# å‡è®¾ä½ å·²ç»å°† PointNet++ çš„æ¨¡å‹ä»£ç æ”¾åœ¨äº† models æ–‡ä»¶å¤¹ä¸‹
# è¿™é‡Œæä¾›ä¸€ä¸ªå ä½å¯¼å…¥ï¼Œä½ éœ€è¦æ ¹æ®ä½ å…‹éš†çš„ä»“åº“å®é™…ç»“æ„ä¿®æ”¹
# from models.pointnet2_sem_seg import get_model, get_loss

def train():
    print("ğŸš€ å¯åŠ¨éš§é“ç®¡é“ PointNet++ è¯­ä¹‰åˆ†å‰²è®­ç»ƒ...")

    # 1. åŸºç¡€é…ç½®
    data_root = "data/processed"
    batch_size = 16
    epochs = 50
    learning_rate = 0.001
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸ å½“å‰ä½¿ç”¨è®¡ç®—è®¾å¤‡: {device}")

    # 2. åŠ è½½æ•°æ®é›†
    # 2. åŠ è½½æ•°æ®é›† (è¿™è¡Œä»£ç æœ¬èº«å…¶å®ä¸ç”¨å¤§æ”¹ï¼Œä¸»è¦æ˜¯å®ƒåƒè¿›å»çš„ data_root å˜äº†)
    train_dataset = TunnelDataset(
        data_root=data_root,
        num_points=4096,  # æ¯æ¬¡å¡ç»™æ˜¾å¡çš„ç‚¹æ•°ï¼Œ4096æ˜¯ç»å…¸é…ç½®
        block_size=3.0,  # ğŸ’¡ è¿›é˜¶å»ºè®®ï¼šæˆ‘æŠŠä½ åŸæ¥çš„ 2.0 æ”¹æˆäº† 3.0
        train=True
    )
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)

    # æ›¿æ¢ train.py ä¸­çš„è¿™ä¸€éƒ¨åˆ†


    # 3. åˆå§‹åŒ–æ¨¡å‹ä¸æŸå¤±å‡½æ•° (2ä¸ªç±»åˆ«ï¼šèƒŒæ™¯=0ï¼Œç®¡é“=1)
    print("ğŸ§  æ­£åœ¨åˆå§‹åŒ– PointNet++ ç½‘ç»œ...")
    model = get_model(num_classes=2).to(device)
    criterion = get_loss().to(device)
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # 4. è®­ç»ƒå¾ªç¯
    os.makedirs("checkpoints", exist_ok=True)
    best_loss = float('inf')

    for epoch in range(epochs):
        model.train()
        total_loss = 0.0

        for batch_idx, (points, labels) in enumerate(train_loader):
            points, labels = points.to(device), labels.to(device)
            optimizer.zero_grad()

            # ã€æ ¸å¿ƒä¿®æ”¹ã€‘ï¼šPyTorch çš„ Conv1d è¦æ±‚è¾“å…¥æ˜¯ [Batch, Channels, N]
            # æˆ‘ä»¬ DataLoader è¾“å‡ºçš„æ˜¯ [Batch, N, 6]ï¼Œæ‰€ä»¥å¿…é¡» transpose ä¸€ä¸‹ï¼
            points = points.transpose(2, 1)

            # å‰å‘ä¼ æ’­
            predictions = model(points)

            # è®¡ç®—æŸå¤±
            loss = criterion(predictions, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

            if batch_idx % 10 == 0:
                print(f"Epoch [{epoch + 1}/{epochs}] Batch [{batch_idx}/{len(train_loader)}] Loss: {loss.item():.4f}")

        # ... åç»­ä¿å­˜æƒé‡çš„é€»è¾‘ä¿æŒä¸å˜ ...

        avg_loss = total_loss / len(train_loader)
        print(f"ğŸ Epoch {epoch + 1} ç»“æŸï¼Œå¹³å‡ Loss: {avg_loss:.4f}")

        # ä¿å­˜æœ€ä¼˜æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save(model.state_dict(), "checkpoints/best_pipe_model.pth")
            print(f"ğŸ’¾ å‘ç°æ›´ä½ Lossï¼Œå·²ä¿å­˜æœ€ä¼˜æ¨¡å‹è‡³ checkpoints/best_pipe_model.pth")


if __name__ == "__main__":
    train()